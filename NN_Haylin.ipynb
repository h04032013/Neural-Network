{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [ 
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creating and then testing a neural network without NN libraries\n",
        "\"\"\"\n",
        "class network:\n",
        "\n",
        "  #Initializing random weight & bias parameters for model\n",
        "  def init_params():\n",
        "  #This example contains 1 hidden layer with \n",
        "  # 2 neurons (W1 and b1, W2 and b2)\n",
        "\n",
        "    #When I tried to remove the -0.5 and change randn to rand,\n",
        "    # i got errors in the epochs/iterations\n",
        "      W1 = np.random.rand(10, 784) - 0.5\n",
        "      #Generate random values from 1-10\n",
        "      b1 = np.random.rand(10, 1) - 0.5 \n",
        "      W2 = np.random.rand(10, 10) - 0.5\n",
        "      b2 = np.random.rand(10, 1) - 0.5\n",
        "      return W1, b1, W2, b2\n",
        "\n",
        "  #Relu activation from scratch\n",
        "  def Relu(Z):\n",
        "      return np.maximum(Z, 0)\n",
        "\n",
        "  #Defining softmax to calulate output (as probability)\n",
        "  def softmax(Z):\n",
        "      A = np.exp(Z) / sum(np.exp(Z))\n",
        "      return A\n",
        "\n",
        "  #Forward propagation\n",
        "#`predict`: Calculates the output values for a list of input data.\n",
        "  def feed_forward(W1, b1, W2, b2, X):\n",
        "      #Calculating output using input, weights & biases\n",
        "      Z1 = W1.dot(X) + b1\n",
        "      #Activate using Relu as activation function\n",
        "      A1 = network.Relu(Z1)\n",
        "      Z2 = W2.dot(A1) + b2\n",
        "      #Getting probability output\n",
        "      A2 = network.softmax(Z2)\n",
        "      return Z1, A1, Z2, A2\n",
        "\n",
        "  #Finding derivative for backwards prop. \n",
        "  # booleans can convert to 1 or 0\n",
        "  def Relu_deriv(Z):\n",
        "      return Z > 0\n",
        "\n",
        "  # ONE-HOT Encoding - Coverting/encoding previous output\n",
        "  # for backwards prop\n",
        "  def encode(Y):\n",
        "    encode_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    #Encode by converting row specified by Y to 1\n",
        "    encode_Y[np.arange(Y.size), Y] = 1\n",
        "    encode_Y = encode_Y.T\n",
        "    return encode_Y\n",
        "\n",
        "  #Backwards prop. to evaluate best paramters using previous output\n",
        "  def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "      m = Y.size\n",
        "      encode_Y = network.encode(Y)\n",
        "      dZ2 = A2 - encode_Y\n",
        "      dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "      db2 = 1 / m * np.sum(dZ2)\n",
        "      #Applying weights in reverse\n",
        "      dZ1 = W2.T.dot(dZ2) * network.Relu_deriv(Z1)\n",
        "      #Booleans can convert to 1 or 0 for multip.\n",
        "\n",
        "      dW1 = 1 / m * dZ1.dot(X.T)\n",
        "      db1 = 1 / m * np.sum(dZ1)\n",
        "      return dW1, db1, dW2, db2\n",
        "\n",
        "  #Update new weight/bias values evaluated by backwards prop.\n",
        "  def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, a):\n",
        "      W1 = W1 - a * dW1\n",
        "      b1 = b1 - a * db1    \n",
        "      W2 = W2 - a * dW2  \n",
        "      b2 = b2 - a * db2    \n",
        "      #Return new/update parameters\n",
        "      return W1, b1, W2, b2\n",
        "\n",
        "  def get_accuracy(predictions, Y):\n",
        "      print(predictions, Y)\n",
        "      #Evaluating output by counting correct predictions\n",
        "      return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "  def get_predictions(A2):\n",
        "      return np.argmax(A2, 0)\n",
        "\n",
        "  #Train model using stochastic gradient descent\n",
        "  def fit(X, Y, a, iterations):\n",
        "      #initialize wieght/bias params for new model\n",
        "      W1, b1, W2, b2 = network.init_params()\n",
        "\n",
        "      for i in range(iterations):\n",
        "        #Loop of Calulating output by feeding forward,\n",
        "        # evaluating ouput & weight/bias parameters using backwards prop., and\n",
        "        # then updating better evaluated params\n",
        "          Z1, A1, Z2, A2 = network.feed_forward(W1, b1, W2, b2, X)\n",
        "          dW1, db1, dW2, db2 = network.backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "          W1, b1, W2, b2 = network.update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, a)\n",
        "          #Show iteration # and accuracy every 25 iterations\n",
        "          if i % 25 == 0:\n",
        "              print(\"Iteration: \", i)\n",
        "              predictions = network.get_predictions(A2)\n",
        "              print(network.get_accuracy(predictions, Y))\n",
        "      return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "rzdCsfewqdW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSm_u-O6V7dh"
      },
      "source": [
        "#Numpy will still be needed for math/linear algebra operations\n",
        "import numpy as np\n",
        "#Pandas will still  be needed for reading data to use the network\n",
        "import pandas as pd\n",
        "#NO Tensorflow/Keras\n",
        "\n",
        "#Using pandas to read sample mnist dataset\n",
        "mnist_data = pd.read_csv('mnist_train_small.csv')\n",
        "\n",
        "#Changing data from pandas dataframe into\n",
        "# a numpy array of values makes it easier to \n",
        "# mathematically manipulate the data with numpy\n",
        "mnist_data = np.array(mnist_data)\n",
        "\n",
        "#Getting the dimensions of the dataset\n",
        "m, n = mnist_data.shape\n",
        "\n",
        "#Shuffling data before splitting into \n",
        "# regular training set and validation set\n",
        "np.random.shuffle(mnist_data) \n",
        "\n",
        "#Splitting data without using test_train_split\n",
        "val_data = mnist_data[0:1000].T \n",
        "#Transposing this would be easier to understand \n",
        "# and work with since there are 700+ pixel values\n",
        "# representing one row/example\n",
        "\n",
        "#Column of correct labels for validation later\n",
        "Y_validation = mnist_data[0] \n",
        "X_validation = val_data[1:n]\n",
        "#Dividing into float values to make them readable as pixels\n",
        "X_validation = X_validation / 255.\n",
        "\n",
        "#Training data\n",
        "data_train = mnist_data[1000:m].T\n",
        "Y_train = data_train[0]\n",
        "X_train = data_train[1:n]\n",
        "X_train = X_train / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling a model to test on sample mnist data set\n",
        "# mnist train/test data as params, 500 iterations for fit loop\n",
        "W1, b1, W2, b2 = network.fit(X_train, Y_train, 0.10, 500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWKac1XR7VlB",
        "outputId": "f7244615-e462-450a-9d0a-4ef1cc54f23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "[9 9 9 ... 4 3 3] [7 3 4 ... 0 4 9]\n",
            "0.10179483130691089\n",
            "Iteration:  25\n",
            "[6 3 3 ... 0 3 3] [7 3 4 ... 0 4 9]\n",
            "0.33175430285804514\n",
            "Iteration:  50\n",
            "[6 3 9 ... 0 4 3] [7 3 4 ... 0 4 9]\n",
            "0.5134480762145376\n",
            "Iteration:  75\n",
            "[8 3 4 ... 0 4 4] [7 3 4 ... 0 4 9]\n",
            "0.6051897468287805\n",
            "Iteration:  100\n",
            "[3 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.668087794094426\n",
            "Iteration:  125\n",
            "[3 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.715985051844834\n",
            "Iteration:  150\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.7502500131585873\n",
            "Iteration:  175\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.7736722985420286\n",
            "Iteration:  200\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.7903573872309069\n",
            "Iteration:  225\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8050423706510869\n",
            "Iteration:  250\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.816464024422338\n",
            "Iteration:  275\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8244118111479551\n",
            "Iteration:  300\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8315174482867519\n",
            "Iteration:  325\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8378862045370808\n",
            "Iteration:  350\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.842728564661298\n",
            "Iteration:  375\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8471498499921049\n",
            "Iteration:  400\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8508868887836202\n",
            "Iteration:  425\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8541502184325491\n",
            "Iteration:  450\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8583609663666508\n",
            "Iteration:  475\n",
            "[7 3 4 ... 0 4 9] [7 3 4 ... 0 4 9]\n",
            "0.8609400494762882\n"
          ]
        }
      ]
    }
  ]
}
